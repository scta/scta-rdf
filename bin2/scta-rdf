#!/usr/bin/env ruby
require "thor"
require "json"
require "nokogiri"
require "open-uri"

require_relative "config"

# class Nokogiri::XML::Document
#   def remove_empty_lines!
#     self.xpath("//text()").each { |text| text.content = text.content.gsub(/\n(\s*\n)+/,"\n") }; self
#   end
# end

class MyCLI < Thor
  desc "clear_TDB", "clear TDB, set prebuild to 'all' to avoid using canvas-prebuild"
  def clear_TDB (buildname, prebuild=true, build_directory=$fuseki_builds)
    exec("#{$base}/bin2/clearTDB.sh #{buildname} #{build_directory} #{prebuild}")
  end
  desc "create_TDB BUILD_DIRECTORY", "create new build director at BUILD_DIRECTORY"
  def create_TDB (buildname, prebuild=true, build_directory=$fuseki_builds)
    exec("#{$base}/bin2/createTDB.sh #{buildname} #{build_directory} #{prebuild}")
  end
  desc "start_fuseki", "start fuseki"
  def start_fuseki (tdb, readonly=false)
    location = "#{$fuseki_builds}/#{tdb}"
    # if readonly
    #   exec("cd #{$fuseki}/ && ./fuseki-server --loc=#{location}  /ds")
    # else
    #   exec("cd #{$fuseki}/ && ./fuseki-server --loc=#{location}  --update /ds")
    # end
    if readonly
      exec("cd #{$fuseki}/ && ./fuseki-server --set tdb:unionDefaultGraph=true --loc=#{location}  /ds")
    else
      exec("cd #{$fuseki}/ && ./fuseki-server --set tdb:unionDefaultGraph=true --loc=#{location}  --update /ds")
    end
  end
  desc "create_manual_ttls", "moves manual ttls from data to build folder"
  def create_manual_ttls
    puts "moving ttl files from data to build folder"
    `cp #{$base}/data/scta-misc-data/*.ttl #{$base}/build`
    puts "moving ttl from scta-rdf-schema"
    `cp #{$base}/data/scta-rdf-schema/src/classes-list.ttl #{$base}/build`
    `cp #{$base}/data/scta-rdf-schema/src/properties-list.ttl #{$base}/build`
  end
  desc "create_canvases", "moves canvases from data to build folder"
  def create_canvases
    puts "moving canvas files from data to build"
    `rm -rf #{$base}/build/canvases/`
    `cp -R #{$base}/data/scta-codices/canvases/ #{$base}/build/canvases/`
  end
  desc "create_codex CODEX", "create codex"
  def create_codex (codex)
    puts "creating codex metadata for #{codex}";
    codex_status = self.same_hash?("#{$base}/data/scta-codices", "#{codex}.xml", "codex")
    if !codex_status
      puts "running codex creation for #{codex}"
      `#{$saxoncommand} '-warnings:silent' '-s:#{$base}/data/scta-codices/#{codex}.xml' '-xsl:#{$base}/xsl_stylesheets/rdf_codices_conversion.xsl' '-o:#{$base}/build/codices/#{codex}.rdf'`
      puts "updating codex status hash in log"
      self.get_hash("#{$base}/data/scta-codices", "#{codex}.xml", "codex")
    else
      puts "no change in codex"
    end

  end
  desc "create_codices", "create codices"
  def create_codices
    Dir.foreach("#{$base}/data/scta-codices/") do |item|
      if item.end_with?(".xml")
        codex = item.split(".xml").first
        self.create_codex(codex)
      end
    end
  end
  desc "create_institution INSTITUTION", "create_institution INSTITUTION"
  def create_institution (institution)
    puts "creating institution metadata for #{institution}";
    institution_status = self.same_hash?("#{$base}/data/scta-codices/institutions", "#{institution}.xml", "institution")
    if !institution_status
      puts "running institution creation for #{institution}"
      `#{$saxoncommand} '-warnings:silent' '-s:#{$base}/data/scta-codices/institutions/#{institution}.xml' '-xsl:#{$base}/xsl_stylesheets/rdf_institution_conversion.xsl' '-o:#{$base}/build/institutions/#{institution}.rdf'`
      puts "updating institution status hash in log"
      self.get_hash("#{$base}/data/scta-codices/institutions", "#{institution}.xml", "institution")
    else
      puts "no change in institution"
    end

  end
  desc "create_institutions", "create institutions"
  def create_institutions
    Dir.foreach("#{$base}/data/scta-codices/institutions/") do |item|
      if item.end_with?(".xml")
        institution = item.split(".xml").first
        self.create_institution(institution)
      end
    end
  end
  desc "create_passive_relation EDF", "create passive relation"
  def create_passive_relation (edf)
    puts "Creating passive metadata assertion for #{edf}"
    commentary_rel_rdf_status = self.same_hash?("#{$base}/build/commentaries", "#{edf}", "commentary-rel-rdf")
    if !commentary_rel_rdf_status
    puts "running passive relations creation for #{edf}"
    `#{$saxoncommand} "-warnings:silent" "-s:#{$base}/build/commentaries/#{edf}" "-xsl:#{$base}/xsl_stylesheets/rdf_relations2_conversion.xsl" "-o:#{$base}/build/relations/rel-#{edf}";`
    puts "updating passive relations status hash in log"
    self.get_hash("#{$base}/build/commentaries", "#{edf}", "commentary-rel-rdf")
    else
      puts "no change in codex"
    end
  end
  desc "create_passive_relations", "create passive relations"
  def create_passive_relations
    Dir.foreach("#{$base}/build/commentaries/") do |item|
      if item.end_with?(".rdf")
        self.create_passive_relation(item)
      end
    end
  end
  desc "create_passive_relations_query", "create passive relations via SPARQL construct query"
  def create_passive_relations_query (property)
    if property == "quotes"
      passive_property = "quotedBy"
    elsif property == "copies"
      passive_property = "copiedBy"
    elsif property == "references"
      passive_property = "referencedBy"
    elsif property == "abbreviates"
      passive_property = "abbreviatedBy"
    elsif property == "isInstanceOf"
      passive_property = "hasInstance"
    elsif property == "isRelatedTo"
      passive_property = "isRelatedTo"
    end
    `curl "http://localhost:3030/ds/query?query=CONSTRUCT+++%7B+%3Fo+%3Chttp%3A%2F%2Fscta.info%2Fproperty%2F#{passive_property}%3E+%3Fs+%7D%0D%0AWHERE+++++++%7B+%3Fs+%3Chttp%3A%2F%2Fscta.info%2Fproperty%2F#{property}%3E+%3Fo+%7D&output=xml&stylesheet=" --create-dirs --output build/relquery/#{property}.rdf`
  end
  desc "create_all_passive_relations_query", "create passive relations via SPARQL construct query"
  def create_all_passive_relations_query
    active_properties = ["quotes", "copies", "references", "abbreviates", "isInstanceOf", "isRelatedTo"]
    active_properties.each do |item|
      self.create_passive_relations_query (item)
    end
  end  
  desc "create_expression EDF", "create expression"
  def create_expression (edf, force=false)
    puts "======"
    puts "creating expression for #{edf}"

    doc = Nokogiri::XML(open("#{$base}/data/scta-projectfiles/#{edf}.xml"))
    text_dirid = doc.xpath("/listofFileNames/header[1]/commentaryid[1]").text
    puts "text_dirid is #{text_dirid}"

    edf_status = self.same_hash?("#{$base}/data/scta-projectfiles", "#{edf}.xml", "edf")
    puts "edf_status #{edf_status}"

    #if Dir.exist?("#{$textfilesbase}#{text_dirid})"
    text_status = self.same_hash?($textfilesbase, text_dirid, "text")
    puts "text_status #{text_status}"
    if (!edf_status || !text_status) || force
      `#{$saxoncommand} -warnings:silent "-s:#{$base}/data/scta-projectfiles/#{edf}.xml" "-xsl:#{$base}/xsl_stylesheets/edf-conversion/main.xsl" "-o:#{$base}/build/commentaries/#{edf}.rdf" "textfilesbase=#{$textfilesbase}";`
      if !edf_status
        puts "updating edf hash in log"
        self.get_hash("#{$base}/data/scta-projectfiles", "#{edf}.xml", "edf")
      end
      if !text_status
        puts "updating text hash in log"
        self.get_hash($textfilesbase, text_dirid, "text")
      end
      puts "========="
    else
      puts "no changes"
      puts "========="
    end
  end
  desc "create_expressions", "create expressions"
  def create_expressions (force=false)
    Dir.foreach("#{$base}/data/scta-projectfiles/") do |item|
      if item.end_with?(".xml")
        edf = item.split(".xml").first
        self.create_expression(edf, force)
      end
    end
  end
  desc "create_workgroups", "create workgroups"
  def create_workgroups
    #projectfilesversion= exec("cd #{$base}/data/scta-projectfiles && $( git describe --tags --always)")
    projectfilesversion="test"
    puts "Begin top level archive collection creation"
    `#{$saxoncommand} "-warnings:silent" "-s:#{$base}/data/scta-misc-data/workGroups.xml" "-xsl:#{$base}/xsl_stylesheets/rdf_workgroup_conversion.xsl" "-o:#{$base}/build/scta.rdf" "projectfileshome="#{$base}/data/scta-projectfiles/`
  end
  desc "create_works", "create work list"
  def create_works
    puts "Begin Workcited Metadata extraction";
  	`#{$saxoncommand} "-s:#{$base}/data/lombardpress-lists/workscited.xml/" "-xsl:#{$base}/xsl_stylesheets/rdf_works_conversion.xsl" "-o:#{$base}/build/works/workscited.rdf";`
  end
  desc "create_names", "create names"
  def create_names
  	puts "Begin Nameslist Metadata extraction";
  	`#{$saxoncommand} "-s:#{$base}/data/scta-people/Prosopography.xml" "-xsl:#{$base}/xsl_stylesheets/rdf_names_conversion.xsl" "-o:#{$base}/build/names/Prosopography.rdf" "workscitedrdf=#{$base}/build/works/workscited.rdf" "scta-base=#{$base}";`
  end
  desc "create_article", "create articles for specific article list"
  def create_article (file, force=false)
    puts "Begin Article Metadata Creation";
    articles_status = self.same_hash?("#{$base}/data/scta-articles", "#{file}.xml", "articles")
    if !articles_status || force
      puts "running articles creation for #{file}"
      `#{$saxoncommand} "-s:#{$base}/data/scta-articles/#{file}.xml" "-xsl:#{$base}/xsl_stylesheets/rdf_article_conversion.xsl" "-o:#{$base}/build/articles/#{file}.rdf";`
      puts "updating codex status hash in log"
      self.get_hash("#{$base}/data/scta-articles", "#{file}.xml", "articles")
    else
      puts "no change in quotation list"
    end
  end
  desc "create_articles", "create articles for all article lists"
  def create_articles (force=false)
    Dir.foreach("#{$base}/data/scta-articles/") do |item|
      if item.end_with?(".xml")
        filename = item.split(".xml").first
        self.create_article(filename, force)
      end
    end
  end
  desc "create_person_groups", "create person groups"
  def create_person_groups
    puts "Begin PersonGroupList Metadata extraction";
  	`#{$saxoncommand} "-s:#{$base}/data/lombardpress-lists/persongroups.xml/" "-xsl:#{$base}/xsl_stylesheets/rdf_persongroups_conversion.xsl" "-o:#{$base}/build/names/persongroups.rdf" ;`
  end
  desc "create_subject_list", "create subject list"
  def create_subject_list
  	puts "Begin Subjectlist Metadata extraction";
  	`#{$saxoncommand} "-s:#{$base}/data/lombardpress-lists/subjectlist.xml/" "-xsl:#{$base}/xsl_stylesheets/rdf_subjects_conversion.xsl" "-o:#{$base}/build/subjects/subjectlist.rdf";`
  end
  desc "update_all", "updates all data"
  def update_all
    #update data repos
    # data repos update first, especially projectfiles/edf
    # edfs are used for pull text repos, so projectfile/edfs need to be updated first
    puts "updating data repos"
    #self.update_repos
    self.update_data_repos
    # updating texts
    puts "updating data text repos"
    self.update_text_repos
    # copying manual ttls from scta-misc-data to build
    puts "copying over manual ttls"
    self.create_manual_ttls
    # copying jsonld canvases from scta-codices/canvaes to build
    puts "copying over manual ttls"
    self.create_canvases
    # end updates
  end
  desc "extract_all", "extract all"
  def extract_all
    # begin xslt creations
    self.create_expressions
    self.create_codices
    self.create_institutions
    self.create_articles
    self.create_workgroups
    self.create_works
    self.create_names
    self.create_person_groups
    self.create_subject_list
  end
  desc "build_and_update", "performs full build and update pattern; assumes fuseki is running"
  def build_and_update
    # update data repos first
    self.update_all

    # delete discarded graphs goes first to remove no longer wanted graph names
    # the deletion occurs before extract all, because the data has precedence over disccarded graph list
    # if the list of discarded graphs contains the same name as a graph that results from extraction, the newly extracted graph will remain
    # graph deletion is only meant to remove old graphs from previous build. it should not delete graphs that are made by the current "data state"
    self.delete_discarded_graphs
    # begin data updates and extraction
    self.extract_all
    # begin adding data to fuseki indexed graph
    self.update_all_graphs
    # perform query on existing indexed graph to get passive relations
    self.create_all_passive_relations_query
    # add new passive relations to indexed graph
    self.update_graphs("relquery")
  end
  desc "delete_graph graph", "delete graph"
  def delete_graph (graph)
    puts "deleting #{graph}"
    `cd #{$fuseki} && ./bin/s-delete http://localhost:3030/ds/data #{graph}`
    self.delete_graph_file(graph)
    puts "removing graph entry from log"
    self.remove_hash(graph)
  end
  desc "delete_graph_file", "delete any file with matching name in any directory within the build folder"
  def delete_graph_file (filename)
    Dir.glob("#{$base}/build/**/#{filename}").each { |f|
      puts "deleting #{f}"
      File.delete(f)
    }
  end
  desc "delete_discarded_graphs graph", "delete graphs listed in log discardedgraphs.json file"
  def delete_discarded_graphs (force=false)
    Dir.foreach("#{$base}/data/scta-misc-data/discarded-graphs") do |item|
      if item.end_with?(".json")
        discarded_graphs_status = self.same_hash?("#{$base}/data/scta-misc-data/discarded-graphs", item, "discarded-graphs")
        puts "test #{discarded_graphs_status}"
        if !discarded_graphs_status || force
          fileObject = File.read("#{$base}/data/scta-misc-data/discarded-graphs/#{item}")
          data_array = JSON.parse(fileObject)
          data_array.each do |graph|
            self.delete_graph(graph)
          end
          puts "updating discarded graphs in logs"
          self.get_hash("#{$base}/data/scta-misc-data/discarded-graphs", item, "discarded-graphs")
        else
          puts "no change discarded graphs list"
        end
      end
    end
  end

  desc "load_graph graph", "load graph"
  def load_graph (subdirectory, graph)
    puts "loading #{$base}/build/#{subdirectory}/#{graph}"
    `cd #{$fuseki} && ./bin/s-post http://localhost:3030/ds/data #{graph} #{$base}/build/#{subdirectory}/#{graph}`
  end
  desc "load_graphs", "load graphs"
  def load_graphs(subdirectory)
    Dir.foreach("#{$base}/build/#{subdirectory}/") do |item|
      if item.end_with?(".rdf") || item.end_with?(".ttl")
        self.load_graph(subdirectory, item)
      end
    end
  end
  # dry run will check changes without actually updating data base 
  # logging will show or silence put messages. 
  # to do snapshot testing, set dry run to true and logging to false. This will render an output for only those graphs that have changed.
  desc "update_graph graph", "update graph"
  def update_graph (subdirectory, graph, force=false, dryrun=false, logging=true)
    ## ensure that boolean parameters are booleans
    force = force.to_s.downcase == "true"
    dryrun = dryrun.to_s.downcase == "true"
    logging = logging.to_s.downcase == "true"
    ##Begin path build
    ##TODO move this switch to a utility file
    full_dir_path = "#{$base}/build/#{subdirectory}"
    if subdirectory == "commentaries"
      type = "texts-rdf"
      status = self.same_hash?(full_dir_path, graph, type, logging)
    elsif subdirectory == "codices"
      type = "codices-rdf"
      status = self.same_hash?(full_dir_path, graph, type, logging)
    elsif subdirectory == "institutions"
      type = "institutions-rdf"
      status = self.same_hash?(full_dir_path, graph, type, logging)
    elsif subdirectory == "names"
      type = "names-rdf"
      status = self.same_hash?(full_dir_path, graph, type, logging)
    elsif subdirectory == "quotations"
      type = "quotations-rdf"
      status = self.same_hash?(full_dir_path, graph, type, logging)
    elsif subdirectory == "quotations/bsvquotations"
      type = "bible-quotations-rdf"
      status = self.same_hash?(full_dir_path, graph, type, logging)
    elsif subdirectory == "relations"
      type = "relations-rdf"
      status = self.same_hash?(full_dir_path, graph, type, logging)
    elsif subdirectory == "relquery"
      type = "relquery-rdf"
      status = self.same_hash?(full_dir_path, graph, type, logging)
    elsif subdirectory == "subjects"
      type = "subjects-rdf"
      status = self.same_hash?(full_dir_path, graph, type, logging)
    elsif subdirectory == "works"
      type = "works-rdf"
      status = self.same_hash?(full_dir_path, graph, type, logging)
    elsif subdirectory == "canvases"
      type = "canvases-rdf"
      status = self.same_hash?(full_dir_path, graph, type, logging)
    elsif subdirectory == "articles"
      type = "articles-rdf"
      status = self.same_hash?(full_dir_path, graph, type, logging)
    else
      type = "misc-rdf"
      status = self.same_hash?(full_dir_path, graph, type, logging)
    end
    ### setting dry run to true simple reports if there has been a change but does not update
    if dryrun
      if !status 
        # no logging conditional here because the only thing this conditional does is log
        # this if we are doing a dry run then we want to see the log
        puts "#{graph} has changed"
      end
    else
      if (logging == true) then puts "updating #{$base}/build/#{subdirectory}/#{graph}" end
      if (logging == true) then puts "status: #{status}" end 
      if !status || force
        if (logging == true) then puts "updating #{graph}" end
        `cd #{$fuseki} && ./bin/s-put http://localhost:3030/ds/data #{graph} #{$base}/build/#{subdirectory}/#{graph}`
        if (logging == true) then puts "updating text-rdf hash in log" end
        self.get_hash(full_dir_path, graph, type)
      else
        if (logging == true) then puts "no changes" end
      end
    end
    
  end
  desc "update_graphs", "update graphs"
  def update_graphs(subdirectory, force=false, dryrun=false, logging=true)
    Dir.foreach("#{$base}/build/#{subdirectory}/") do |item|
      if item.end_with?(".rdf") || item.end_with?(".ttl") || item.end_with?(".jsonld")
        self.update_graph(subdirectory, item, force, dryrun, logging)
      end
    end
  end
  desc "update_graph_toplevel", "update top level text graph and all sub items"
  def update_graph_toplevel(edf, force=false, dryrun=false, logging=true)

    doc = Nokogiri::XML(open("#{$base}/data/scta-projectfiles/#{edf}.xml"))
    
    self.update_graph("commentaries", edf + ".rdf", force, dryrun, logging)
    itemnames = doc.xpath("/listofFileNames//item/fileName/@filestem")
    itemnames.each do |item|
      if (logging == true) then puts "updating #{item}" end
      self.update_graph("commentaries", item.to_s + ".rdf", force, dryrun, logging)
    end
  end
  desc "update_all_graphs", "update graphs"
  def update_all_graphs
    self.update_graphs("codices")
    self.update_graphs("institutions")
    self.update_graphs("canvases")
    self.update_graphs("commentaries")
    self.update_graphs("names")
    self.update_graphs("quotations")
    self.update_graphs("quotations/bsvquotations")
    self.update_graphs("articles")
    self.update_graphs("subjects")
    self.update_graphs("works")


    #relquery can be skipped because it is always going to be called separately after passive relation creation
    #self.update_graphs("relquery")

    # miscellaneous graph updates
    self.update_graph("", "scta_expression_types.ttl")
    self.update_graph("", "scta_author_types.ttl")
    self.update_graph("", "articles.ttl")
    self.update_graph("", "scta.rdf")

    self.update_graph("", "classes-list.ttl")
    self.update_graph("", "properties-list.ttl")

  end
  desc "update_canvas", "load canvas"
  def update_canvas (graph)
    puts "loading canvas #{$base}/data/scta-codices/canvases/#{graph}"
    `cd #{$fuseki} && ./bin/s-put http://localhost:3030/ds/data #{graph} #{$base}/build/canvases/#{graph}`
  end
  desc "update_canvases", "load canvases"
  def update_canvases
    puts "loading canvases in #{$base}/data/scta-codices/canvases/"
    Dir.foreach("#{$base}/data/scta-codices/canvases/") do |item|
      if item.end_with?(".jsonld")
        self.load_canvas(item)
      end
    end
  end

  desc "load_canvas", "load canvas"
  def load_canvas (graph)
    puts "loading canvas #{$base}/data/scta-codices/canvases/#{graph}"
    `cd #{$fuseki} && ./bin/s-post http://localhost:3030/ds/data #{graph} #{$base}/data/scta-codices/canvases/#{graph}`
  end
  desc "load_canvases", "load canvases"
  def load_canvases
    puts "loading canvases in #{$base}/data/scta-codices/canvases/"
    Dir.foreach("#{$base}/data/scta-codices/canvases/") do |item|
      if item.end_with?(".jsonld")
        self.load_canvas(item)
      end
    end
  end
  desc "load_all", "load all"
  def load_all
    #exec("./loadFusekiRdf.sh #{$base} #{$fuseki}")
    self.load_graph("", "scta.rdf")
    self.load_graphs("commentaries")
    self.load_graphs("names")
    self.load_graphs("works")
    self.load_graphs("subjects")
    self.load_graphs("quotations")
    self.load_graphs("quotations/bsvquotations")
    self.load_graphs("relquery")
    self.load_graphs("codices")
    self.load_graphs("institutions")
    # these data sources should get copied into build
    # and then imported from build to keep things consistent
    self.load_graph("", "classes-list.ttl")
    self.load_graph("", "properties-list.ttl")
    self.load_graph("", "scta_expression_types.ttl")
    self.load_graph("", "scta_author_types.ttl")
    self.load_graph("", "articles.ttl")
    #canvases should be loaded in preload

  end
  desc "update_repos", "update all data repos"
  def update_repos
    Dir.foreach("#{$base}/data/") do |file|
      next if file == '.' or file == '..'
      if File.exist?("#{$base}/data/#{file}/.git")
        puts file
        self.update_repo(file)
      end
    end
  end
  desc "update_repo", "update a single repo"
  def update_repo(repo)
    puts repo
    `cd #{$base}/data/#{repo} && git pull`
  end

  desc "update_data_repo", "updates a data repo for a given github url "
  def update_data_repo (datarepobase, datarepo)
  `mkdir -p #{$base}/data && \
      curl -L -o #{$base}/data/#{datarepo}.tar https://api.github.com/repos/#{datarepobase}/#{datarepo}/tarball/master && \
      tar -xvzf #{$base}/data/#{datarepo}.tar --directory #{$base}/data/
      mv #{$base}/data/#{datarepobase}-#{datarepo}* #{$base}/data/#{datarepo} && \
      rm -rf #{$base}/data/#{datarepobase}-#{datarepo}*`

    `rm #{$base}/data/#{datarepo}.tar`

  end
desc "update_data_repos", "updates all data repos besides texts "
  def update_data_repos ()
    puts "test"
    datarepos = [
      "scta-projectfiles",
      "scta-codices", 
      "scta-rdf-schema", 
      "scta-people", 
      "scta-articles", 
      "scta-misc-data",
      "lombardpress-lists"
    ]
    datarepos.each do |item|
      puts item
      datarepobase = item == "lombardpress-lists" ? "lombardpress" : "scta"
      update_data_repo(datarepobase, item) 
    end  
  end
  desc "update_scta_texts", "updates scta_texts data folder from url pointing to tarball"
  def update_scta_texts (url, ipfs=false)
    if ipfs == "true"
      url = "http://gateway.ipfs.io/ipfs/#{url}"
    end
    puts url
    `mkdir -p #{$base}/data/scta-texts && \
    curl -L -o #{$base}/data/scta-texts/tmp-scta-texts.tar #{url} && \
    tar -xvzf #{$base}/data/scta-texts/tmp-scta-texts.tar -C #{$base}/data/scta-texts/`
  end

  desc "update_text_repo", "updates a single scta-text repo from url point to tarball"
  def update_text_repo (textrepo)
  `mkdir -p #{$base}/data/scta-texts && \
      curl -L -o #{$base}/data/scta-texts/#{textrepo}.tar https://api.github.com/repos/scta-texts/#{textrepo}/tarball/master && \
      tar -xvzf #{$base}/data/scta-texts/#{textrepo}.tar --directory #{$base}/data/scta-texts/ && \
      rm -rf #{$base}/data/scta-texts/#{textrepo} && \
      mv #{$base}/data/scta-texts/scta-texts-#{textrepo}* #{$base}/data/scta-texts/#{textrepo} && \
      rm -rf #{$base}/data/scta-texts/scta-texts-#{textrepo}*`

    `rm #{$base}/data/scta-texts/#{textrepo}.tar`

  end
  desc "update_text_repos", "updates all texts repos using ids from edf/projectfiles"
  def update_text_repos

    Dir.foreach("#{$base}/data/scta-projectfiles/") do |item|
      if item.end_with?(".xml")
        edf = item.split(".xml").first
        doc = Nokogiri::XML(open("#{$base}/data/scta-projectfiles/#{edf}.xml"))
        reponame = doc.xpath("/listofFileNames/header[1]/commentaryid[1]").text
        puts "text_dirid is #{reponame}"
        puts "attempting tarball retrieval for #{reponame}"
        self.update_text_repo(reponame)
      end
    end
  end

  desc "get hash", "creates hash for target and adds to specified hash table"
  def get_hash (directory, target, type)

    if type == "text"
      shasum_response = `cd #{$textfilesbase} && find #{target}/ -type f -print0 | sort -z | xargs -0 #{$shacommand} | #{$shacommand}`
      shasum = shasum_response.split(" ")[0]
    else
      shasum_response = `#{$shacommand} #{directory}/#{target}`
      shasum = shasum_response.split(" ")[0]
    end


    if File.exist?("#{$base}/logs/#{type}-hashes.json")
      if !File.zero?("#{$base}/logs/#{type}-hashes.json")
        file = File.read("#{$base}/logs/#{type}-hashes.json")
        data_hash = JSON.parse(file)
      else
        data_hash = {}
      end
    else
      data_hash = {}
    end

    data_hash[target] = shasum

    # make sure logs folder exists before writing
    if !File.exist?("#{$base}/logs/")
      Dir.mkdir "#{$base}/logs/"
    end
    # write new hash to file
    File.open("#{$base}/logs/#{type}-hashes.json","w+") do |f|
      f.write(JSON.pretty_generate(data_hash))
    end
  end
  desc "get hashes", "creates hash table for targets and adds to specified hash table"
  def get_hashes (subdirectory, type)
    if type == "text"
      Dir.foreach("#{$textfilesbase}") do |dir|
        next if dir == '.' or dir == '..'
        puts dir
        get_hash($textfilesbase, dir, type)
      end
    else
      Dir.foreach("#{$base}/#{subdirectory}") do |file|
        next if file == '.' or file == '..'
        puts file
        get_hash("#{$base}/#{subdirectory}", file, type)
      end
    end
  end
  desc "same hash?", "compares files and return true if the hashes are the same and false if they are different"
  def same_hash? (directory, target, type, logging=true)
    if type == "text"
      shasum_response = `cd #{$textfilesbase} && find #{target}/ -type f -print0 | sort -z | xargs -0 #{$shacommand} | #{$shacommand}`
      shasum = shasum_response.split(" ")[0]
    else
      shasum_response = `#{$shacommand} #{directory}/#{target}`
      shasum = shasum_response.split(" ")[0]
    end
    if (logging == true) then puts "new shasum #{shasum}" end

    if File.exist?("#{$base}/logs/#{type}-hashes.json")
      file = File.read("#{$base}/logs/#{type}-hashes.json")
      data_hash = JSON.parse(file)
      if (logging == true) then puts "old shasum #{data_hash[target]}" end
      status = data_hash[target] ==  shasum
    else
      if (logging == true) then puts "no existing file" end
      status = false
    end
    if (logging == true) then puts status end
    return status
  end
  desc "remove_hash", "remove a hash from any log file"
  def remove_hash (hashname)
    Dir.foreach("#{$base}/logs") do |file|
      next if file == '.' or file == '..' or file == 'build-logs'
      fileObject = File.read("#{$base}/logs/#{file}")
      data_hash = JSON.parse(fileObject)
      puts "removes #{hashname} from #{file}"
      data_hash.delete(hashname) do |el|
        puts "#{el} not found"
      end
      # write new hash to file
      File.open("#{$base}/logs/#{file}","w+") do |f|
        f.write(JSON.pretty_generate(data_hash))
      end
    end
  end

  desc "split_large_files", "splits large build files into smaller versions"
  def split_large_files
    large_files = true
    while large_files
      Dir.foreach("#{$base}/build/commentaries/") do |item|
        if item.end_with?(".rdf")
          size = File.size("#{$base}/build/commentaries/#{item}") / 1024000
          #puts "#{item}: #{size}"
          if size > 75
            puts "size is greater than 75; beginning file split"
            doc = Nokogiri::XML(open("#{$base}/build/commentaries/#{item}"))
            count = doc.xpath("count(//rdf:Description)", 'rdf' => 'http://www.w3.org/1999/02/22-rdf-syntax-ns#').floor

            puts "#{item} description count: #{count}"

            splitSize = 4
            splitSize.times do |index|
              puts "index: #{index}"
              keepStart = index * (count/splitSize).floor
              keepStop = index == (splitSize - 1) ? count : (index + 1) * (count/splitSize).floor
              puts "keepstart: #{keepStart}; keepstop: #{keepStop}"
              c = doc.clone
              (c.xpath("/rdf:RDF/rdf:Description", 'rdf' => 'http://www.w3.org/1999/02/22-rdf-syntax-ns#') - c.xpath("/rdf:RDF/rdf:Description", 'rdf' => 'http://www.w3.org/1999/02/22-rdf-syntax-ns#')[keepStart...keepStop]).remove

              itemslug = item.split(".rdf").first
              File.open("#{$base}/build/commentaries/#{itemslug}-#{index}.rdf", 'w') do |f|
                f.write(c.to_s)
              end
            end
            newfilename = "#{$base}/build/commentaries/#{item}".gsub(".rdf", ".xml")
            File.rename("#{$base}/build/commentaries/#{item}", newfilename)

            if File.exist?("#{$base}/logs/discardedgraphs.json")
              if !File.zero?("#{$base}/logs/discardedgraphs.json")
                file = File.read("#{$base}/logs/discardedgraphs.json")
                data_hash = JSON.parse(file)
              else
                data_hash = []
              end
            else
              data_hash = []
            end

            data_hash << item

            # make sure logs folder exists before writing
            if !File.exist?("#{$base}/logs/")
              Dir.mkdir "#{$base}/logs/"
            end
            # write new has to file
            File.open("#{$base}/logs/discardedgraphs.json","w+") do |f|
              f.write(JSON.pretty_generate(data_hash))
            end
          else
            large_files = false
          end

        end
      end
    end
  end



end

MyCLI.start(ARGV)
